{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis and Entity Resolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "DATAFILE_PATTERN = '^(.+),\"(.+)\",(.*),(.*),(.*)'\n",
    "\n",
    "def removeQuotes(s):\n",
    "    \"\"\" Remove quotation marks from an input string\n",
    "    Args:\n",
    "        s (str): input string that might have the quote \"\" characters\n",
    "    Returns:\n",
    "        str: a string without the quote characters\n",
    "    \"\"\"\n",
    "    return ''.join(i for i in s if i != '\"')\n",
    "\n",
    "def parseDatafileLine(datafileLine):\n",
    "    \"\"\" Parse a line of the data file using the specified regualr expression pattern\n",
    "    Args:\n",
    "        datafileLine (str): input string that is a line from the data file\n",
    "    Returns:\n",
    "        str: a string parsed using the given regular expression and without the quote characters\n",
    "    \"\"\"\n",
    "    match = re.search(DATAFILE_PATTERN, datafileLine)\n",
    "    if match is None:\n",
    "        print('Invalid datafile line: {0}'.format(datafileLine))\n",
    "        return (datafileLine, -1)\n",
    "    elif match.group(1) == '\"id\"':\n",
    "        print('Header datafile line: {0}'.format(datafileLine))\n",
    "        return (datafileLine, 0)\n",
    "    else:\n",
    "        product = '%s %s %s' % (match.group(2), match.group(3), match.group(4))\n",
    "        return ((removeQuotes(match.group(1)), product), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google_small.csv - Read 201 lines, successfully parse 200 lines, failed to parse 0 lines\n",
      "Google.csv - Read 3227 lines, successfully parse 3226 lines, failed to parse 0 lines\n",
      "Amazon_small.csv - Read 201 lines, successfully parse 200 lines, failed to parse 0 lines\n",
      "Amazon.csv - Read 1364 lines, successfully parse 1363 lines, failed to parse 0 lines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_dir = os.path.join('user', 'root', 'lab3')\n",
    "GOOGLE_PATH = 'Google.csv'\n",
    "GOOGLE_SMALL_PATH = 'Google_small.csv'\n",
    "AMAZON_PATH = 'Amazon.csv'\n",
    "AMAZON_SMALL_PATH = 'Amazon_small.csv'\n",
    "GOLD_STANDARD_PATH = 'Amazon_Google_perfectMapping.csv'\n",
    "STOPWORDS_PATH = 'stopwords.txt'\n",
    "\n",
    "def parseData(filename):\n",
    "    \"\"\" Parse a data file\n",
    "    Args:\n",
    "        filename (str): input file name of the data file\n",
    "    Returns:\n",
    "        RDD: an RDD of parsed lines\n",
    "    \"\"\"\n",
    "    return (sc\n",
    "            .textFile(filename, 4, 0)\n",
    "            .map(parseDatafileLine)\n",
    "            .cache())\n",
    "def loadData(path):\n",
    "    \"\"\" Load a data file\n",
    "    Args:\n",
    "        path (str): input file name of the data file\n",
    "    Returns:\n",
    "        RDD: an RDD of parsed valid lines \n",
    "    \"\"\"\n",
    "    filename = 'hdfs:/' + os.path.join(data_dir, path)\n",
    "    raw = parseData(filename).cache()\n",
    "    failed = (raw\n",
    "              .filter(lambda s: s[1] == -1)\n",
    "              .map(lambda s: s[0])\n",
    "              .cache())\n",
    "    for line in failed.take(10):\n",
    "        print('%s - Invalid datafile line: %s' % (path, line))\n",
    "    valid = (raw\n",
    "             .filter(lambda s: s[1] == 1)\n",
    "             .map(lambda s: s[0])\n",
    "             .cache())\n",
    "    print('{0} - Read {1} lines, successfully parse {2} lines, failed to parse {3} lines'\n",
    "          .format(path, raw.count(), valid.count(), failed.count()))\n",
    "    assert failed.count() == 0\n",
    "    assert raw.count() == (valid.count()+1)\n",
    "    return valid\n",
    "\n",
    "googleSmall = loadData(GOOGLE_SMALL_PATH)\n",
    "google = loadData(GOOGLE_PATH)\n",
    "amazonSmall = loadData(AMAZON_SMALL_PATH)\n",
    "amazon = loadData(AMAZON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google: http://www.google.com/base/feeds/snippets/11448761432933644608: spanish vocabulary builder \"expand your vocabulary! contains fun lessons that both teach and entertain you'll quickly find yourself mastering new terms. includes games and more!\" \n",
      "\n",
      "google: http://www.google.com/base/feeds/snippets/8175198959985911471: topics presents: museums of world \"5 cd-rom set. step behind the velvet rope to examine some of the most treasured collections of antiquities art and inventions. includes the following the louvre - virtual visit 25 rooms in full screen interactive video detailed map of the louvre ...\" \n",
      "\n",
      "google: http://www.google.com/base/feeds/snippets/18445827127704822533: sierrahome hse hallmark card studio special edition win 98 me 2000 xp \"hallmark card studio special edition (win 98 me 2000 xp)\" \"sierrahome\"\n",
      "\n",
      "amazon: b000jz4hqo: clickart 950 000 - premier image pack (dvd-rom)  \"broderbund\"\n",
      "\n",
      "amazon: b0006zf55o: ca international - arcserve lap/desktop oem 30pk \"oem arcserve backup v11.1 win 30u for laptops and desktops\" \"computer associates\"\n",
      "\n",
      "amazon: b00004tkvy: noah's ark activity center (jewel case ages 3-8)  \"victory multimedia\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in googleSmall.take(3):\n",
    "    print('google: {0}: {1}\\n'.format(line[0], line[1]))\n",
    "\n",
    "for line in amazonSmall.take(3):\n",
    "    print('amazon: {0}: {1}\\n'.format(line[0], line[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ER as Text Similarity - Bags of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) Tokenize a String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "[]\n",
      "['123a', '456_b', '789c', '123a']\n",
      "['fox', 'fox']\n"
     ]
    }
   ],
   "source": [
    "quickbrownfox = 'A quick brown fox jumps over the lazy dog.'\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "def simpleTokenize(string):\n",
    "    \"\"\" A simple implementation of input string tokenization\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens\n",
    "    \"\"\"\n",
    "    return [t for t in re.split(split_regex, string.lower()) if len(t)]\n",
    "\n",
    "#test cases\n",
    "print(simpleTokenize(quickbrownfox))\n",
    "print(simpleTokenize(' '))\n",
    "print(simpleTokenize('!!!!123A/456_B/789C.123A'))\n",
    "print(simpleTokenize('fox fox'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the stopwords: set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'with', u'had', u'should', u'to', u'only', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'did', u'these', u't', u'each', u'where', u'because', u'doing', u'theirs', u'some', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'below', u'does', u'above', u'between', u'she', u'be', u'we', u'after', u'here', u'hers', u'by', u'on', u'about', u'of', u'against', u's', u'or', u'own', u'into', u'yourself', u'down', u'your', u'from', u'her', u'whom', u'there', u'been', u'few', u'too', u'themselves', u'was', u'until', u'more', u'himself', u'that', u'but', u'off', u'herself', u'than', u'those', u'he', u'me', u'myself', u'this', u'up', u'will', u'while', u'can', u'were', u'my', u'and', u'then', u'is', u'in', u'am', u'it', u'an', u'as', u'itself', u'at', u'have', u'further', u'their', u'if', u'again', u'no', u'when', u'same', u'any', u'how', u'other', u'which', u'you', u'who', u'most', u'such', u'why', u'a', u'don', u'i', u'having', u'so', u'the', u'yours', u'once'])\n",
      "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
      "[]\n",
      "['the_']\n"
     ]
    }
   ],
   "source": [
    "stopfile = os.path.join('lab3', STOPWORDS_PATH)\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "print('These are the stopwords: %s' % stopwords)\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    return [t for t in re.split(split_regex, string.lower()) if len(t) and t not in stopwords]\n",
    "\n",
    "#test cases\n",
    "print(tokenize(quickbrownfox))\n",
    "print(tokenize('Why a the?'))\n",
    "print(tokenize('Being at the_?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) Tokenizing the small datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22520 tokens in the combined datasets\n"
     ]
    }
   ],
   "source": [
    "amazonRecToToken = amazonSmall.map(lambda x:(x[0], tokenize(x[1])))\n",
    "googleRecToToken = googleSmall.map(lambda x:(x[0], tokenize(x[1])))\n",
    "\n",
    "def countTokens(vendorRDD):\n",
    "    \"\"\" Count and return the number of tokens\n",
    "    Args:\n",
    "        vendorRDD (RDD of (recordId, tokenizedValue)): Pair tuple of record ID to tokenized output\n",
    "    Returns:\n",
    "        count: count of all tokens\n",
    "    \"\"\"\n",
    "    recordCount = vendorRDD.map(lambda s:len(s[1]))\n",
    "    recordSum = recordCount.reduce(lambda a, b:a+b)\n",
    "    return recordSum\n",
    "\n",
    "totalTokens = countTokens(amazonRecToToken) + countTokens(googleRecToToken)\n",
    "print('There are {0} tokens in the combined datasets'.format(totalTokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d) Amazon record with the most tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Amazon record with ID \"b000o24l3q\" has the most tokens (1547)\n"
     ]
    }
   ],
   "source": [
    "def findBiggestRecord(vendorRDD):\n",
    "    \"\"\" Find and return the record with the largest number of tokens\n",
    "    Args:\n",
    "        vendorRDD (RDD of (recordID, tokens): input Pair Tuple of record ID and tokens)\n",
    "    Returns:\n",
    "        list: a list of 1 Pair Tuple of record ID and tokens\n",
    "    \"\"\"\n",
    "    return (vendorRDD.takeOrdered(1, lambda s: -1 * len(s[1])))\n",
    "\n",
    "biggestRecordAmazon = findBiggestRecord(amazonRecToToken)\n",
    "print('The Amazon record with ID \"{0}\" has the most tokens ({1})'.format(biggestRecordAmazon[0][0], len(biggestRecordAmazon[0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: ER as Text Similarity - Weighted Bag-of-Words using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF: Term Frequency\n",
    "$TF=\\frac{T_a}{T_{total}}$\n",
    "* $T_a$: number of times that token $t$ appears in a document $d$\n",
    "* $T_{total}$: total number of tokens in a document $d$\n",
    "\n",
    "### IDF: Inverse-Document-Frequency\n",
    "$IDF=\\frac{N}{n(t)}$\n",
    "* $N$: total number of documents in a set of documents $U$.\n",
    "* $n(t)$: the number of documents in $U$ that contain $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) Implement a TF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brown': 0.16666666666666666, 'lazy': 0.16666666666666666, 'jumps': 0.16666666666666666, 'fox': 0.16666666666666666, 'dog': 0.16666666666666666, 'quick': 0.16666666666666666}\n",
      "{'two': 0.3333333333333333, 'one_': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "def tf(tokens):\n",
    "    \"\"\" Compute TF\n",
    "    Args:\n",
    "        tokens (list of str): input list of tokens from tokenize\n",
    "    Returns:\n",
    "        dictionary: a dictionary of tokens to its TF values\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    length = len(tokens)\n",
    "    for t in tokens:\n",
    "        counts.setdefault(t, 0.0)\n",
    "        counts[t] += 1\n",
    "    return {t:counts[t]/length for t in counts}\n",
    "\n",
    "#test cases\n",
    "print(tf(tokenize(quickbrownfox)))\n",
    "print(tf(tokenize('one_ One_ two!')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) Create a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusRDD = amazonRecToToken.union(googleRecToToken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Implement a IDF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4772 unique tokens in the small datasets.\n",
      "software\n",
      "4.25531914894\n"
     ]
    }
   ],
   "source": [
    "def idfs(corpus):\n",
    "    \"\"\" Compute IDF\n",
    "    Args:\n",
    "        corpus (RDD): input corpus\n",
    "    Returns:\n",
    "        RDD: an RDD of (token, IDF value)\n",
    "    \"\"\"\n",
    "    N = float(corpus.count())\n",
    "    uniqueTokens = corpus.flatMap(lambda s: set(s[1]))\n",
    "    tokenCountPairTuple = uniqueTokens.map(lambda s: (s,1))\n",
    "    tokenSumPairTuple = tokenCountPairTuple.reduceByKey(lambda x, y: x+y)\n",
    "    return (tokenSumPairTuple.map(lambda s: (s[0], float(N/s[1]))))\n",
    "\n",
    "idfsSmall = idfs(amazonRecToToken.union(googleRecToToken))\n",
    "uniqueTokenCount = idfsSmall.count()\n",
    "\n",
    "print('There are {0} unique tokens in the small datasets.'.format(uniqueTokenCount))\n",
    "\n",
    "tokenSmallestIdf = idfsSmall.takeOrdered(1, lambda s: s[1])[0]\n",
    "print(tokenSmallestIdf[0])\n",
    "print(tokenSmallestIdf[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2d) Tokens with the smallest IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('software', 4.25531914893617), ('new', 6.896551724137931), ('features', 6.896551724137931), ('use', 7.017543859649122), ('complete', 7.2727272727272725), ('easy', 7.6923076923076925), ('create', 8.333333333333334), ('system', 8.333333333333334), ('cd', 8.333333333333334), ('1', 8.51063829787234), ('windows', 8.51063829787234)]\n"
     ]
    }
   ],
   "source": [
    "smallIDFTokens = idfsSmall.takeOrdered(11, lambda s: s[1])\n",
    "print(smallIDFTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2e) IDF Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAADFCAYAAACb4LFtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nADAwQQ0AAxPUADAwQQ0AA/t/gtYQ9fx3cDQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9588fe3e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "small_idf_values = idfsSmall.map(lambda s: s[1]).collect()\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.hist(small_idf_values, 50, log=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|      token|             value|\n",
      "+-----------+------------------+\n",
      "|      aided|             400.0|\n",
      "|    precise|             100.0|\n",
      "|     duplex|             400.0|\n",
      "|      dance|             400.0|\n",
      "|     breath|             200.0|\n",
      "|        edr|             400.0|\n",
      "|      known|             100.0|\n",
      "|     verses|             400.0|\n",
      "|         go|              80.0|\n",
      "|       9999|             400.0|\n",
      "|      album|             400.0|\n",
      "|     layers|             100.0|\n",
      "|   increase| 66.66666666666667|\n",
      "|     themes|133.33333333333334|\n",
      "|        xml|             400.0|\n",
      "|         tv|             200.0|\n",
      "|     volume|              80.0|\n",
      "|environment| 44.44444444444444|\n",
      "|     german|             400.0|\n",
      "|      topic|             400.0|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "idfsToCountRow = idfsSmall.map(lambda (x,y):(x,y))\n",
    "idfsToCountDF = sqlContext.createDataFrame(idfsToCountRow, ('token', 'value'))\n",
    "idfsToCountDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2f) Implement a TF-IDF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon record \"b000hkgj8k\" has tokens and weights:\n",
      "{'autocad': 33.33333333333333, 'autodesk': 8.333333333333332, 'courseware': 66.66666666666666, 'psg': 33.33333333333333, '2007': 3.5087719298245617, 'customizing': 16.666666666666664, 'interface': 3.0303030303030303}\n"
     ]
    }
   ],
   "source": [
    "def tfidf(tokens, idfs):\n",
    "    \"\"\" Compute TF-IDF\n",
    "    Args:\n",
    "        tokens (list of str): inputlist of tokens from tokenize\n",
    "        idfs (dictionary): record to IDF value\n",
    "    Returns:\n",
    "        dictionary: a dictionary of records to TF-IDF values\n",
    "    \"\"\"\n",
    "    tfs = tf(tokens)\n",
    "    tfIdfDict = {t: tfs[t] * idfs[t] for t in tfs}\n",
    "    return tfIdfDict\n",
    "\n",
    "recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]\n",
    "idfsSmallWeights = idfsSmall.collectAsMap()\n",
    "rec_b000hkgj8k_weights = tfidf(recb000hkgj8k, idfsSmallWeights)\n",
    "print('Amazon record \"b000hkgj8k\" has tokens and weights:\\n{0}'.format(rec_b000hkgj8k_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: ER as Text Similarity - Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is defined as\n",
    "\\begin{equation}\n",
    "\\text{similarity}=\\cos(\\theta)=\\frac{a\\cdot b}{|a||b|}=\\frac{\\sum a_ib_i}{\\sqrt{\\sum a_i^2}\\sqrt{\\sum b_i^2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Implement the components of a `cosineSimilarity` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102, 6.164414002968976)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def dotprod(a, b):\n",
    "    \"\"\" Compute dot product\n",
    "    Args:\n",
    "        a (dictionary): first dictionary of record to value\n",
    "        b (dictionary): second dictionary of record to value\n",
    "    Returns:\n",
    "        dotProd: result of the dot product with the tow input dictionaries\n",
    "    \"\"\"\n",
    "    return sum([a[t] * b[t] for t in a if t in b])\n",
    "\n",
    "def norm(a):\n",
    "    \"\"\" Compute square root of the dot product\n",
    "    Args:\n",
    "        a (dictionary): a dictionary of record to value\n",
    "    Returns:\n",
    "        norm (float): the square root of the dot product value\n",
    "    \"\"\"\n",
    "    return math.sqrt(dotprod(a, a))\n",
    "\n",
    "def cossim(a,b):\n",
    "    \"\"\" Compute cosine similarity\n",
    "    Args:\n",
    "        a (dictionary): first dictionary of record to value\n",
    "        b (dictionary): second dictionary of record to value\n",
    "    Returns:\n",
    "        cossim (float): cosine similarity\n",
    "    \"\"\"\n",
    "    return dotprod(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "# test cases\n",
    "testVec1 = {1:2, 2:3, 3:5}\n",
    "testVec2 = {1:1, 2:0, 3:20}\n",
    "dp = dotprod(testVec1, testVec2)\n",
    "nm = norm(testVec1)\n",
    "print(dp, nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Implement a `cosineSimilarity` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0577243382163\n"
     ]
    }
   ],
   "source": [
    "def cosineSimilarity(string1, string2, idfsDictionary):\n",
    "    \"\"\" Compute cosine similarity between two strings\n",
    "    Args:\n",
    "        string1 (str): first string\n",
    "        string2 (str): second string\n",
    "        idfsDictionary (dictionary): a dictionary of IDF values\n",
    "    Returns:\n",
    "        cossim (float): cosine similarity value\n",
    "    \"\"\"\n",
    "    w1 = tfidf(tokenize(string1), idfsDictionary)\n",
    "    w2 = tfidf(tokenize(string2), idfsDictionary)\n",
    "    return cossim(w1, w2)\n",
    "\n",
    "# test case\n",
    "cossimAdobe = cosineSimilarity('Adobe Photoshop',\n",
    "                               'Adobe Illustrator',\n",
    "                               idfsSmallWeights)\n",
    "print(cossimAdobe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c) Perform Entity Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested similarity is 0.000303171940451\n"
     ]
    }
   ],
   "source": [
    "crossSmall = (googleSmall\n",
    "              .cartesian(amazonSmall)\n",
    "              .cache())\n",
    "def computeSimilarity(record):\n",
    "    \"\"\" Compute similarity on a combination record\n",
    "    Args:\n",
    "        record: a pair, (google record, amazon record)\n",
    "    Returns:\n",
    "        pair: a pair, (google URL, amazon ID, cosine similarity value)\n",
    "    \"\"\"\n",
    "    googleRec = record[0]\n",
    "    amazonRec = record[1]\n",
    "    googleURL = googleRec[0]\n",
    "    amazonID = amazonRec[0]\n",
    "    googleValue = googleRec[1]\n",
    "    amazonValue = amazonRec[1]\n",
    "    cs = cosineSimilarity(googleValue, amazonValue, idfsSmallWeights)\n",
    "    return (googleURL, amazonID, cs)\n",
    "\n",
    "similarities = (crossSmall\n",
    "                .map(computeSimilarity)\n",
    "                .cache())\n",
    "\n",
    "def similar(amazonID, googleURL):\n",
    "    \"\"\" Return similarity value\n",
    "    Args:\n",
    "        amazonID: amazon ID\n",
    "        googleURL: google URL\n",
    "    Returns:\n",
    "        similar: cosine similarity value\n",
    "    \"\"\"\n",
    "    return (similarities\n",
    "            .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))\n",
    "            .collect()[0][2])\n",
    "\n",
    "similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')\n",
    "print('Requested similarity is {0}'.format(similarityAmazonGoogle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Perform Entity Resolution with Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested similarity is 0.000303171940451\n"
     ]
    }
   ],
   "source": [
    "def computeSimilarityBroadcast(record):\n",
    "    \"\"\" Compute similarity on a combination record, using Broadcast variable\n",
    "    Args:\n",
    "        record: a pair, (google record, amazon record)\n",
    "    Returns:\n",
    "        pair: a pair, (google URL, amazon ID, cosine similarity value)\n",
    "    \"\"\"\n",
    "    googleRec = record[0]\n",
    "    amazonRec = record[1]\n",
    "    googleURL = googleRec[0]\n",
    "    amazonID = amazonRec[0]\n",
    "    googleValue = googleRec[1]\n",
    "    amazonValue = amazonRec[1]\n",
    "    cs = cosineSimilarity(googleValue, amazonValue, idfsSmallBroadcast.value)\n",
    "    return (googleURL, amazonID, cs)\n",
    "\n",
    "idfsSmallBroadcast = sc.broadcast(idfsSmallWeights)\n",
    "similaritiesBroadcast = (crossSmall\n",
    "                         .map(computeSimilarityBroadcast)\n",
    "                         .cache())\n",
    "\n",
    "def similarBoradcast(amazonID, googleURL):\n",
    "    \"\"\" Return similarity value, computed using Broadcast variable\n",
    "    Args:\n",
    "        amazonID: amazon ID\n",
    "        googleURL: google URL\n",
    "    Returns:\n",
    "        similar: cosine similarity\n",
    "    \"\"\"\n",
    "    return (similaritiesBroadcast\n",
    "            .filter(lambda record: record[0] == googleURL and record[1] == amazonID)\n",
    "            .collect()[0][2])\n",
    "similarityAmazonGoolgeBroadcast = similarBoradcast('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')\n",
    "print('Requested similarity is {0}'.format(similarityAmazonGoolgeBroadcast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3e) Perform a Gold Standard Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1301 lines, successfully parsed 1300 lines, failed to parse 0 lines\n"
     ]
    }
   ],
   "source": [
    "GOLDFILE_PATTERN = '^(.+),(.+)'\n",
    "\n",
    "def parse_goldfile_line(goldfile_line):\n",
    "    \"\"\" Parse a line from the 'golden standard' data file\n",
    "    Args:\n",
    "        goldfile_line: a line of data\n",
    "    Returns:\n",
    "        pair: ((key, 'gold'), 1 if successful or else 0))\n",
    "    \"\"\"\n",
    "    match = re.search(GOLDFILE_PATTERN, goldfile_line)\n",
    "    if match is None:\n",
    "        print('Invalid goldfile line: {0}'.format(goldfile_line))\n",
    "        return (goldfile_line, -1)\n",
    "    elif match.group(1) == '\"idAmazon\"':\n",
    "        print('Header datafile line: {0}'.format(goldfile_line))\n",
    "        return (goldfile_line, 0)\n",
    "    else:\n",
    "        key = '{0} {1}'.format(removeQuotes(match.group(1)), removeQuotes(match.group(2)))\n",
    "        return ((key, 'gold'), 1)\n",
    "    \n",
    "goldfile = os.path.join('lab3', GOLD_STANDARD_PATH)\n",
    "gsRaw = (sc\n",
    "         .textFile(goldfile)\n",
    "         .map(parse_goldfile_line)\n",
    "         .cache())\n",
    "gsFailed = (gsRaw\n",
    "            .filter(lambda s: s[1] == -1)\n",
    "            .map(lambda s: s[0]))\n",
    "for line in gsFailed.take(10):\n",
    "    print('Invalid goldfile line: {0}'.format(line))\n",
    "\n",
    "goldStandard = (gsRaw\n",
    "                .filter(lambda s: s[1] == 1)\n",
    "                .map(lambda s: s[0])\n",
    "                .cache())\n",
    "\n",
    "print('Read {0} lines, successfully parsed {1} lines, failed to parse {2} lines'.format(gsRaw.count(),\n",
    "                                                                                        goldStandard.count(),\n",
    "                                                                                        gsFailed.count()))\n",
    "assert gsFailed.count() == 0\n",
    "assert gsRaw.count() == (goldStandard.count()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = similaritiesBroadcast.map(lambda x:('{0} {1}'.format(x[1], x[0]), x[2]))\n",
    "\n",
    "trueDupsRDD = (sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
