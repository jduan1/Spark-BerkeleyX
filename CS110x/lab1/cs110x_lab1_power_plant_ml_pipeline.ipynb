{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" align=\"left\" /></a><br><br/>This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Plant Machine Learning Pipeline Application\n",
    "This notebook is an end-to-end exercise of performing Extract-Transform-Load and Exploratory Data Analysis on a real-world dataset, and then applying several different machine learning algorithms to solve a supervised regression problem on the dataset.\n",
    "\n",
    "**This notebook covers:**\n",
    "* Part 1: Business Understanding\n",
    "* Part 2: Load Your Data\n",
    "* Part 3: Explore Your Data\n",
    "* Part 4: Visualize Your Data\n",
    "* Part 5: Data Preparation\n",
    "* Part 6: Data Modeling\n",
    "* Part 7: Tuning and Evaluation\n",
    "\n",
    "_Our goal is to accurately predict power output given a set of environmental readings from various sensors in a natural gas-fired power generation plant._\n",
    "\n",
    "**Background**\n",
    "\n",
    "Power generation is a complex process, and understanding and predicting power output is an important element in managing a plant and its connection to the power grid. The operators of a regional power grid create predictions of power demand based on historical information and environmental factors (e.g., temperature). They then compare the predictions against available resources (e.g., coal, natural gas, nuclear, solar, wind, hydro power plants). Power generation technologies such as solar and wind are highly dependent on environmental conditions, and all generation technologies are subject to planned and unplanned maintenance.\n",
    "\n",
    "Here is an real-world example of predicted demand (on two time scales), actual demand, and available resources from the California power grid: http://www.caiso.com/Pages/TodaysOutlook.aspx\n",
    "\n",
    "![](http://content.caiso.com/outlook/SP/ems_small.gif)\n",
    "\n",
    "The challenge for a power grid operator is how to handle a shortfall in available resources versus actual demand. There are three solutions to a power shortfall: build more base load power plants (this process can take many years to decades of planning and construction), buy and import power from other regional power grids (this choice can be very expensive and is limited by the power transmission interconnects between grids and the excess power available from other grids), or turn on small [Peaker or Peaking Power Plants](https://en.wikipedia.org/wiki/Peaking_power_plant). Because grid operators need to respond quickly to a power shortfall to avoid a power outage, grid operators rely on a combination of the last two choices. In this exercise, we'll focus on the last choice.\n",
    "\n",
    "**The Business Problem**\n",
    "\n",
    "Because they supply power only occasionally, the power supplied by a peaker power plant commands a much higher price per kilowatt hour than power from a power grid's base power plants. A peaker plant may operate many hours a day, or it may operate only a few hours per year, depending on the condition of the region's electrical grid. Because of the cost of building an efficient power plant, if a peaker plant is only going to be run for a short or highly variable time it does not make economic sense to make it as efficient as a base load power plant. In addition, the equipment and fuels used in base load plants are often unsuitable for use in peaker plants because the fluctuating conditions would severely strain the equipment.\n",
    "\n",
    "The power output of a peaker power plant varies depending on environmental conditions, so the business problem is predicting the power output of a peaker power plant as a function of the environmental conditions -- since this would enable the grid operator to make economic tradeoffs about the number of peaker plants to turn on (or whether to buy expensive power from another grid).\n",
    "\n",
    "Given this business problem, we need to first perform Exploratory Data Analysis to understand the data and then translate the business problem (predicting power output as a function of envionmental conditions) into a Machine Learning task. In this instance, the ML task is regression since the label (or target) we are trying to predict is numeric. We will use an [Apache Spark ML Pipeline](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark-ml-package) to perform the regression.\n",
    "\n",
    "The real-world data we are using in this notebook consists of 9,568 data points, each with 4 environmental attributes collected from a Combined Cycle Power Plant over 6 years (2006-2011), and is provided by the University of California, Irvine at [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant). You can find more details about the dataset on the UCI page, including the following background publications:\n",
    "* Pinar Tüfekci, [Prediction of full load electrical power output of a base load operated combined cycle power plant using machine learning methods](http://www.journals.elsevier.com/international-journal-of-electrical-power-and-energy-systems/), International Journal of Electrical Power & Energy Systems, Volume 60, September 2014, Pages 126-140, ISSN 0142-0615.\n",
    "* Heysem Kaya, Pinar Tüfekci and Fikret S. Gürgen: [Local and Global Learning Methods for Predicting Power of a Combined Gas & Steam Turbine](http://www.cmpe.boun.edu.tr/~kaya/kaya2012gasturbine.pdf), Proceedings of the International Conference on Emerging Trends in Computer and Electronics Engineering ICETCEE 2012, pp. 13-18 (Mar. 2012, Dubai).\n",
    "\n",
    "**To Do**: Read the documentation and examples for [Spark Machine Learning Pipeline](https://spark.apache.org/docs/1.6.2/ml-guide.html#main-concepts-in-pipelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Business Understanding\n",
    "The first step in any machine learning task is to understand the business need.\n",
    "\n",
    "As described in the overview we are trying to predict power output given a set of readings from various sensors in a gas-fired power generation plant.\n",
    "\n",
    "The problem is a regression problem since the label (or target) we are trying to predict is numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Extract-Transform-Load (ETL) Your Data\n",
    "Now that we understand what we are trying to do, the first step is to load our data into a format we can query and use. This is known as ETL or \"Extract-Transform-Load\". We will load our file from Amazon S3.\n",
    "\n",
    "Note: Alternatively we could upload our data using \"Databricks Menu > Tables > Create Table\", assuming we had the raw files on our local computer.\n",
    "\n",
    "Our data is available on Amazon s3 at the following path:\n",
    "\n",
    "    dbfs:/databricks-datasets/power-plant/data\n",
    "    \n",
    "**To Do**: Let's start by printing a sample of the data.\n",
    "\n",
    "We'll use the built-in Databricks functions for exploring the Databricks filesystem (DBFS)\n",
    "\n",
    "Use `display(dbutils.fs.ls(\"/databricks-datasets/power-plant/data\"))` to list the files in the directory\n",
    "\n",
    "**Task Omitted. We will work on the data provided on the cluster**\n",
    "\n",
    "    hdfs:/user/root/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2(a)\n",
    "Now, let's use PySpark instead to print the first 5 lines of the data.\n",
    "\n",
    "_Hint_: First create an RDD from the data by using `sc.textFile(\"hdfs:/user/root/data\")` to read the data into an RDD.\n",
    "\n",
    "_Hint_: Then figure out how to use the RDD `take()` method to extract the first 5 lines of the RDD and print each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawTextRdd = sc.textFile('hdfs:/user/root/data')\n",
    "print(rawTextRdd.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our initial exploration of a sample of the data, we can make several observations for the ETL process:\n",
    "* The data is a set of .tsv (Tab Seperated Values) files (i.e., each row of the data is separated using tabs)\n",
    "* There is a header row, which is the name of the columns\n",
    "* It looks like the type of the data in each column is consistent (i.e., each column is of type double)\n",
    "\n",
    "Our schema definition from UCI appears below:\n",
    "* AT = Atmospheric Temperature in C\n",
    "* V = Exhaust Vacuum Speed\n",
    "* AP = Atmospheric Pressure\n",
    "* RH = Relative Humidity\n",
    "* PE = Power Output. This is the value we are trying to predict given the measurements above.\n",
    "\n",
    "We are ready to create a DataFrame from the TSV data. Spark does not have a native method for performing this operation, however we can use [spark-csv](https://spark-packages.org/package/databricks/spark-csv), a third-party package from [SparkPackages](https://spark-packages.org/). The documentation and source code for [spark-csv](https://spark-packages.org/package/databricks/spark-csv) can be found on [GitHub](https://github.com/databricks/spark-csv). The Python API can be found [here](https://github.com/databricks/spark-csv#python-api).\n",
    "\n",
    "(**Note**: In Spark 2.0, the CSV package is built into the DataFrame API.)\n",
    "\n",
    "To use the spark-csv package, we use the `sqlContext.read.format()` method to specify the input data source format: 'com.databricks.spark.csv'\n",
    "We can provide the spark-csv package with options using the `options()` method. The available options are listed in the GitHub documentation [here](https://github.com/databricks/spark-csv#features).\n",
    "\n",
    "We will use the following three options:\n",
    "* `delimiter='\\t'` because our data is tab delimited\n",
    "* `header='true'` because our data has a header row\n",
    "* `inferschema='true'` because we believe that all of the data is double values, so the package can dynamically infer the type of each column. Note that this will require two pass over the data.\n",
    "\n",
    "The last component of creating the DataFrame is to specify the location of the data source using the load() method: `\"hdfs:/user/root/data\"`\n",
    "\n",
    "Putting everything together, we will use an operation of the following form:\n",
    "\n",
    "    sqlContext.read.format().options().load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2(b)\n",
    "**To Do**: Create a DataFrame from the data.\n",
    "\n",
    "_Hint_: Use the above template and fill in each of the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "powerPlantDF = (sqlContext.read\n",
    "                .format('com.databricks.spark.csv')\n",
    "                .option('delimiter','\\t')\n",
    "                .option('header',True)\n",
    "                .option('inferschema',True)\n",
    "                .load('/user/root/data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the names and types of the columns using the dtypes method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(powerPlantDF.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerPlantDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Alternative Method to Load your Data\n",
    "Instead of having [spark-csv](https://spark-packages.org/package/databricks/spark-csv) infer the types of the columns, we can specify the schema as a [DataType](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.types.DataType), which is a list of [StructField](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.types.StructType).\n",
    "\n",
    "You can find a list of types in the [pyspark.sql.types](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#module-pyspark.sql.types) module. For our data, we will use [DoubleType()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.types.DoubleType).\n",
    "\n",
    "For example, to specify that a column's name and type, we use: `StructField(name, type, True)`. (The third parameter, `True`, signifies that the column is nullable.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2(c)\n",
    "Create a custom schema for the power plant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "customSchema = StructType([\\\n",
    "                          StructField('AT', DoubleType(), True), \\\n",
    "                          StructField('V', DoubleType(), True), \\\n",
    "                          StructField('AP', DoubleType(), True), \\\n",
    "                          StructField('RH', DoubleType(), True), \\\n",
    "                          StructField('PE', DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2(d)\n",
    "Now, let's use the schema to read the data. To do this, we will modify the earlier `sqlContext.read.format` step. We can specify the schema by:\n",
    "* Adding `schema = customSchema` to the load method (use a comma and add it after the file name)\n",
    "* Removing the `inferschema='true'` option because we are explicitly specifying the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "altPowerPlantDF = (sqlContext.read\n",
    "                   .format('com.databricks.spark.csv')\n",
    "                   .option('delimiter', '\\t')\n",
    "                   .option('header', True)\n",
    "                   .load('/user/root/data', schema = customSchema))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that no Spark jobs are launched this time. That is because we specified the schema, so the spark-csv package does not have to read the data to infer the schema. We can use the dtypes method to examine the names and types of the columns. They should be identical to the names and types of the columns that were earlier inferred from the data.\n",
    "\n",
    "When you run the following cell, data would not be read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(altPowerPlantDF.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can examine the data. Note that this operation will cause the data to be read and the DataFrame will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altPowerPlantDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Explore Your Data\n",
    "Now that your data is loaded, the next step is to explore it and perform some basic analysis and visualizations.\n",
    "\n",
    "This is a step that you should always perform **before** trying to fit a model to the data, as this step will often lead to important insights about your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's register our DataFrame as an SQL table named `power_plant`. Because you may run this lab multiple times, we'll take the precaution of removing any existing tables first.\n",
    "\n",
    "We can delete any existing power_plant SQL table using the SQL command: `DROP TABLE IF EXISTS power_plant` (we also need to to delete any Hive data associated with the table, which we can do with a Databricks file system operation).\n",
    "\n",
    "Once any prior table is removed, we can register our DataFrame as a SQL table using [sqlContext.registerDataFrameAsTable()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.SQLContext.registerDataFrameAsTable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql('DROP TABLE IF EXISTS power_plant')\n",
    "sqlContext.registerDataFrameAsTable(powerPlantDF, 'power_plant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our DataFrame exists as a SQL table, we can explore it using SQL commands.\n",
    "\n",
    "To execute SQL in a cell, we use the %sql operator. The following cell is an example of using SQL to query the rows of the SQL table.\n",
    "\n",
    "**NOTE**: `%sql` is a Databricks-only command. It calls `sqlContext.sql()` and passes the results to the Databricks-only display() function. These two statements are equivalent:\n",
    "\n",
    "    %sql SELECT * FROM power_plant\n",
    "\n",
    "    display(sqlContext.sql(\"SELECT * FROM power_plant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql('SELECT * FROM power_plant').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(c)\n",
    "Use the SQL `desc` command to describe the schema, by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql('DESC power_plant').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schema Definition**\n",
    "Once again, here's our schema definition:\n",
    "* AT = Atmospheric Temperature in C\n",
    "* V = Exhaust Vacuum Speed\n",
    "* AP = Atmospheric Pressure\n",
    "* RH = Relative Humidity\n",
    "* PE = Power Output\n",
    "\n",
    "PE is our label or target. This is the value we are trying to predict given the measurements.\n",
    "\n",
    "[Reference UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform some basic statistical analyses of all the columns.\n",
    "\n",
    "We can get the DataFrame associated with a SQL table by using the [sqlContext.table()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.table) method and passing in the name of the SQL table. Then, we can use the DataFrame [describe()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) method with no arguments to compute some basic statistics for each column like count, mean, max, min and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.table('power_plant')\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerPlantDF.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualize Your Data\n",
    "This part is omitted because it requires Databrick's build-in visualization tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ASIDE: A quick physics lesson**: This correlation is to be expected as the second law of thermodynamics puts a fundamental limit on the [thermal efficiency](https://en.wikipedia.org/wiki/Thermal_efficiency) of all heat-based engines. The limiting factors are:\n",
    "* The temperature at which the heat enters the engine $T_{H}$\n",
    "* The temperature of the environment into which the engine exhausts its waste heat $T_C$\n",
    "\n",
    "Our temperature measurements are the temperature of the environment. From [Carnot's theorem](https://en.wikipedia.org/wiki/Carnot%27s_theorem_%28thermodynamics%29), no heat engine working between these two temperatures can exceed the Carnot Cycle efficiency:\n",
    "\n",
    "\\begin{equation}\n",
    "n_{th} \\le 1 - \\frac{T_C}{T_H}\n",
    "\\end{equation}\n",
    "\n",
    "Note that as the environmental temperature increases, the efficiency decreases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Preparation\n",
    "The next step is to prepare the data for machine learning. Since all of this data is numeric and consistent this is a simple and straightforward task.\n",
    "\n",
    "The goal is to use machine learning to determine a function that yields the output power as a function of a set of predictor features. The first step in building our ML pipeline is to convert the predictor features from DataFrame columns to Feature Vectors using the [pyspark.ml.feature.VectorAssembler()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler) method.\n",
    "\n",
    "The VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. VectorAssembler takes a list of input column names (each is a string) and the name of the output column (as a string).\n",
    "\n",
    "### Exercise 5(a)\n",
    "* Read the Spark documentation and useage examples for [VectorAssembler](https://spark.apache.org/docs/1.6.2/ml-features.html#vectorassembler)\n",
    "* Convert the `power_plant` SQL table into a DataFrame named dataset\n",
    "* Set the vectorizer's input columns to a list of the four columns of the input DataFrame: `[\"AT\", \"V\", \"AP\", \"RH\"]`\n",
    "* Set the vectorizer's output column name to `\"features\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "datasetDF = sqlContext.table('power_plant')\n",
    "\n",
    "vectorizer = VectorAssembler()\n",
    "vectorizer.setInputCols(['AT','V','AP','RH','PE'])\n",
    "vectorizer.setOutputCol('features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Data Modeling\n",
    "Now let's model our data to predict what the power output will be given a set of sensor readings\n",
    "\n",
    "Our first model will be based on simple linear regression since we saw some linear patterns in our data based on the scatter plots during the exploration stage.\n",
    "\n",
    "We need a way of evaluating how well our linear regression model predicts power output as a function of input parameters. We can do this by splitting up our initial data set into a Training Set used to train our model and a Test Set used to evaluate the model's performance in giving predictions. We can use a DataFrame's [randomSplit()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) method to split our dataset. The method takes a list of weights and an optional random seed. The seed is used to initialize the random number generator used by the splitting function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6(a)\n",
    "Use the [randomSplit()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) method to divide up `datasetDF` into a trainingSetDF (80% of the input DataFrame) and a testSetDF (20% of the input DataFrame), and for reproducibility, use the seed 1800009193L. Then cache each DataFrame in memory to maximize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 1800009193L\n",
    "(split20DF, split80DF) = datasetDF.randomSplit([0.2, 0.8],seed)\n",
    "testSetDF = split20DF.cache()\n",
    "trainingSetDF = split80DF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a Linear Regression Model and use the built in help to identify how to train it. See API details for [Linear Regression](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression) in the ML guide.\n",
    "### Exercise 6(b)\n",
    "* Read the documentation and examples for [Linear Regression](https://spark.apache.org/docs/1.6.2/ml-classification-regression.html#linear-regression)\n",
    "* Run the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr = LinearRegression()\n",
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is based on the [Spark ML Pipeline API for Linear Regression](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression).\n",
    "\n",
    "The first step is to set the parameters for the method:\n",
    "* Set the name of the prediction column to \"Predicted_PE\"\n",
    "* Set the name of the label column to \"PE\"\n",
    "* Set the maximum number of iterations to 100\n",
    "* Set the regularization parameter to 0.1\n",
    "\n",
    "Next, we create the [ML Pipeline](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Pipeline) and set the stages to the Vectorizer and Linear Regression learner we created earlier.\n",
    "\n",
    "Finally, we create a model by training on `trainingSetDF`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6(c)\n",
    "* Read the [Linear Regression](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression) documentation\n",
    "* Run the next cell, and be sure you understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we set the parameters for the method\n",
    "lr.setPredictionCol(\"Predicted_PE\")\\\n",
    "  .setLabelCol(\"PE\")\\\n",
    "  .setMaxIter(100)\\\n",
    "  .setRegParam(0.1)\n",
    "\n",
    "\n",
    "# We will use the new spark.ml pipeline API. If you have worked with scikit-learn this will be very familiar.\n",
    "lrPipeline = Pipeline()\n",
    "\n",
    "lrPipeline.setStages([vectorizer, lr])\n",
    "\n",
    "# Let's first train on the entire dataset to see what we get\n",
    "lrModel = lrPipeline.fit(trainingSetDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Wikipedia article on [Linear Regression](https://en.wikipedia.org/wiki/Linear_regression):\n",
    "> In statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable $y$ and one or more explanatory variables (or independent variables) denoted $X$. In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models.\n",
    "\n",
    "Linear regression has many practical uses. Most applications fall into one of the following two broad categories:\n",
    "* If the goal is prediction, or forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of $y$ and $X$ values. After developing such a model, if an additional value of $X$ is then given without its accompanying value of $y$, the fitted model can be used to make a prediction of the value of $y$.\n",
    "* Given a variable yy and a number of variables $X_1, ..., X_p$ that may be related to $y$, linear regression analysis can be applied to quantify the strength of the relationship between $y$ and the $X_j$, to assess which $X_j$ may have no relationship with $y$ at all, and to identify which subsets of the $X_j$ contain redundant information about $y$.\n",
    "\n",
    "We are interested in both uses, as we would like to predict power output as a function of the input variables, and we would like to know which input variables are weakly or strongly correlated with power output.\n",
    "\n",
    "Since Linear Regression is simply a Line of best fit over the data that minimizes the square of the error, given multiple input dimensions we can express each predictor as a line function of the form:\n",
    "\\begin{equation}\n",
    "y = a + b_1x_1 + b_2x_2 + b_3x_i\\cdots\n",
    "\\end{equation}\n",
    "where $a$ is the intercept and the $b_i$ are the coefficients.\n",
    "\n",
    "To express the coefficients of that line we can retrieve the Estimator stage from the PipelineModel and express the weights and the intercept for the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6(d)\n",
    "Run the next cell. Ensure that you understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The intercept is as follows:\n",
    "intercept = lrModel.stages[1].intercept\n",
    "\n",
    "# The coefficents (i.e., weights) are as follows:\n",
    "weights = lrModel.stages[1].coefficients\n",
    "\n",
    "# Create a list of the column names (without PE)\n",
    "featuresNoLabel = [col for col in datasetDF.columns if col != 'PE']\n",
    "\n",
    "# Merge the weights and labels\n",
    "coefficients = zip(weights, featuresNoLabel)\n",
    "\n",
    "# Now let's sort the coefficients from greatest absolute weight most to the least absolute weight\n",
    "coefficients.sort(key=lambda tup:abs(tup[0]), reverse=True)\n",
    "\n",
    "equation = 'y = {intercept}'.format(intercept=intercept)\n",
    "variables = []\n",
    "for x in coefficients:\n",
    "    weights = abs(x[0])\n",
    "    name = x[1]\n",
    "    symbol = '+' if (x[0] > 0) else '-'\n",
    "    equation += ('{} ({} * {})'.format(symbol, weights, name))\n",
    "\n",
    "# Finally here is our equation\n",
    "print('Linear Regression Equation: ' + equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6(e)\n",
    "Now let's see what our predictions look like given this model. We apply our Linear Regression model to the 20% of the data that we split from the input dataset. The output of the model will be a predicted Power Output column named \"Predicted_PE\".\n",
    "* Run the next cell\n",
    "* Scroll through the resulting table and notice how the values in the Power Output (PE) column compare to the corresponding values in the predicted Power Output (Predicted_PE) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply our LR model to the test data and predict power output\n",
    "predictionsAndLabelsDF = lrModel.transform(testSetDF).select('AT', 'V', 'AP', 'RH', 'PE', 'Predicted_PE')\n",
    "predictionsAndLabelsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a visual inspection of the predictions, we can see that they are close to the actual values.\n",
    "\n",
    "However, we would like a scientific measure of how well the Linear Regression model is performing in accurately predicting values. To perform this measurement, we can use an evaluation metric such as [Root Mean Squared Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMSE) to validate our Linear Regression model.\n",
    "\n",
    "RSME is defined as follows: $RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n} (x_i - y_i)^2}{n}}$ where $y_i$ is the observed value and $x_i$ is the predicted value\n",
    "\n",
    "RMSE is a frequently used measure of the differences between values predicted by a model or an estimator and the values actually observed. The lower the RMSE, the better our model.\n",
    "\n",
    "Spark ML Pipeline provides several regression analysis metrics, including [RegressionEvaluator()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator).\n",
    "\n",
    "After we create an instance of [RegressionEvaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator), we set the label column name to \"PE\" and set the prediction column name to \"Predicted_PE\". We then invoke the evaluator on the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6(f)\n",
    "Run the next cell and ensure that you understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compute an evaluation metric for our test dataset\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create an RMSE evaluator using the label and predicted columns\n",
    "regEval = RegressionEvaluator(predictionCol='Predicted_PE', labelCol='PE', metricName='rmse')\n",
    "\n",
    "# Run the evaluator on the DataFrame\n",
    "rmse = regEval.evaluate(predictionsAndLabelsDF)\n",
    "\n",
    "print('Root Mean Squared Error: %.2f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful statistical evaluation metric is the coefficient of determination, denoted $R^2$ or $r^2$ and pronounced \"R squared\". It is a number that indicates the proportion of the variance in the dependent variable that is predictable from the independent variable and it provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model. The coefficient of determination ranges from 0 to 1 (closer to 1), and the higher the value, the better our model.\n",
    "\n",
    "To compute $r^2$, we invoke the evaluator with regEval.metricName: \"r2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6(g)\n",
    "Run the next cell and ensure that you understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compute another evaluation metric for our test dataset\n",
    "r2 = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: 'r2'})\n",
    "print('r2: {0:.2f}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, assuming a Gaussian distribution of errors, a good model will have 68% of predictions within 1 RMSE and 95% within 2 RMSE of the actual value (see http://statweb.stanford.edu/~susan/courses/s60/split/node60.html).\n",
    "\n",
    "Let's examine the predictions and see if a RMSE of 0.32 meets this criteria.\n",
    "\n",
    "We create a new DataFrame using [selectExpr()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.selectExpr) to project a set of SQL expressions, and register the DataFrame as a SQL table using [registerTempTable()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6(h)\n",
    "Run the next cell and ensure that you understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we remove the table if it already exists\n",
    "sqlContext.sql('DROP TABLE IF EXISTS Power_Plant_RMSE_Evalutation')\n",
    "\n",
    "# Next we calculate the residual error and divide it by the RMSE\n",
    "predictionsAndLabelsDF.selectExpr('PE', 'Predicted_PE', 'PE - Predicted_PE Residual_Error', '(PE - Predicted_PE) / {} Within_RSME'.format(rmse)).registerTempTable('Power_Plant_RMSE_Evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use SQL to explore the `Power_Plant_RMSE_Evaluation` table. First let's look at at the table using a SQL SELECT statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6(i)\n",
    "Run the next cell and ensure that you understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql('SELECT * FROM Power_Plant_RMSE_Evaluation').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6(j), 6(k)\n",
    "Requires Databricks build-in visualization tools. Omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Tuning and Evaluation\n",
    "Now that we have a model with all of the data let's try to make a better model by tuning over several parameters. The process of tuning a model is known as [Model Selection](https://spark.apache.org/docs/1.6.2/ml-tuning.html#model-selection-aka-hyperparameter-tuning) or [Hyperparameter Tuning](https://spark.apache.org/docs/1.6.2/ml-tuning.html#model-selection-aka-hyperparameter-tuning), and Spark ML Pipeline makes the tuning process very simple and easy.\n",
    "\n",
    "An important task in ML is model selection, or using data to find the best model or parameters for a given task. This is also called tuning. Tuning may be done for individual Estimators such as [LinearRegression](https://spark.apache.org/docs/1.6.2/ml-classification-regression.html#linear-regression), or for entire Pipelines which include multiple algorithms, featurization, and other steps. Users can tune an entire Pipeline at once, rather than tuning each element in the Pipeline separately.\n",
    "\n",
    "Spark ML Pipeline supports model selection using tools such as CrossValidator, which requires the following items:\n",
    "* [Estimator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Estimator): algorithm or Pipeline to tune\n",
    "* [Set of ParamMaps](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder): parameters to choose from, sometimes called a parameter grid to search over\n",
    "* [Evaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.Evaluator): metric to measure how well a fitted Model does on held-out test data\n",
    "\n",
    "At a high level, model selection tools such as [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation) work as follows:\n",
    "* They split the input data into separate training and test datasets.\n",
    "* For each (training, test) pair, they iterate through the set of ParamMaps:\n",
    "    * For each [ParamMap](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder), they fit the [Estimator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Estimator) using those parameters, get the fitted Model, and evaluate the Model's performance using the [Evaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.Evaluator).\n",
    "* They select the Model produced by the best-performing set of parameters.\n",
    "\n",
    "The [Evaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.Evaluator) can be a [RegressionEvaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator) for regression problems. To help construct the parameter grid, users can use the [ParamGridBuilder](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder) utility.\n",
    "\n",
    "Note that cross-validation over a grid of parameters is expensive. For example, in the next cell, the parameter grid has 10 values for [lr.regParam](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression.regParam), and [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation) uses 3 folds. This multiplies out to (10 x 3) = 30 different models being trained. In realistic settings, it can be common to try many more parameters (e.g., multiple values for multiple parameters) and use more folds (k = 3 and k = 10 are common). In other words, using [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation) can be very expensive. However, it is also a well-established method for choosing parameters which is more statistically sound than heuristic hand-tuning.\n",
    "\n",
    "We perform the following steps:\n",
    "* Create a [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation) using the Pipeline and [RegressionEvaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator) that we created earlier, and set the number of folds to 3\n",
    "* Create a list of 10 regularization parameters\n",
    "* Use [ParamGridBuilder](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder) to build a parameter grid with the regularization parameters and add the grid to the [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation)\n",
    "* Run the [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation) to find the parameters that yield the best model (i.e., lowest RMSE) and return the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7(a)\n",
    "Run the next cell. Note that it will take some time to run the CrossValidator as it will run almost 200 Spark jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# We can reuse the RegressionEvaluator, regEval, to judge the model based on the best Root Mean Squared Error\n",
    "# Let's create our CrossValidator with 3 fold cross validation\n",
    "crossval = CrossValidator(estimator=lrPipeline, evaluator=regEval, numFolds=3)\n",
    "\n",
    "# Let's tune over our regularization parameter from 0.01 to 0.10\n",
    "regParam = [x / 100.0 for x in range(1,11)]\n",
    "\n",
    "# We'll create a paramter grid using the ParamGridBuilder, and add the grid to the CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, regParam)\n",
    "             .build())\n",
    "crossval.setEstimatorParamMaps(paramGrid)\n",
    "\n",
    "# Now let's find and return the best model\n",
    "cvModel = crossval.fit(trainingSetDF).bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tuned our Linear Regression model, let's see what the new RMSE and $r^2$ values are versus our intial model.\n",
    "### Exercise 7(b)\n",
    "Complete and run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use cvModel to compute an evaluation metric for our test dataset: testSetDF\n",
    "predictionsAndLabelsDF = cvModel.transform(testSetDF)\n",
    "\n",
    "# Run the previously created RMSE evaluator, regEval, on the predictionsAndLabelsDF DataFrame\n",
    "regEval = RegressionEvaluator(predictionCol='Predicted_PE', labelCol='PE', metricName='rmse')\n",
    "rmseNew = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: 'rmse'})\n",
    "\n",
    "# Now let's compute the r2 evaluation metric for our test dataset\n",
    "r2New = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: 'r2'})\n",
    "print(\"Original Root Mean Squared Error: {0:2.2f}\".format(rmse))\n",
    "print('New Root Mean Squared Error: {0:2.2f}'.format(rmseNew))\n",
    "print('Old r2: {0:2.2f}'.format(r2))\n",
    "print('New r2: {0:2.2f}'.format(r2New))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our initial untuned and tuned linear regression models are statistically identical. Let's look at the regularization parameter that the [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation) has selected.\n",
    "\n",
    "Recall that the orginal regularization parameter we used was 0.01.\n",
    "\n",
    "**NOTE**: The ML Python API currently doesn't provide a way to query the regularization parameter, so we cheat, by \"reaching through\" to the JVM version of the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Regularization parameter of the best model: {0:0.2f}'.format(cvModel.stages[-1]._java_obj.parent().getRegParam()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the only linearly correlated variable is Temperature, it makes sense try another Machine Learning method such as [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree_learning) to handle non-linear data and see if we can improve our model.\n",
    "\n",
    "[Decision Tree Learning](https://en.wikipedia.org/wiki/Decision_tree_learning) uses a [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree) as a predictive model which maps observations about an item to conclusions about the item's target value. It is one of the predictive modelling approaches used in statistics, data mining and machine learning. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.\n",
    "\n",
    "Spark ML Pipeline provides [DecisionTreeRegressor()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor) as an implementation of [Decision Tree Learning](https://en.wikipedia.org/wiki/Decision_tree_learning).\n",
    "\n",
    "The cell below is based on the [Spark ML Pipeline API for Decision Tree Regressor](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7(c)\n",
    "* Read the [Decision Tree Regressor](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor) documentation\n",
    "* In the next cell, create a [DecisionTreeRegressor()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor)\n",
    "* The next step is to set the parameters for the method (we do this for you):\n",
    "    * Set the name of the prediction column to \"Predicted_PE\"\n",
    "    * Set the name of the features column to \"features\"\n",
    "    * Set the maximum number of bins to 100\n",
    "* Create the [ML Pipeline](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Pipeline) and set the stages to the Vectorizer we created earlier and [DecisionTreeRegressor()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor) learner we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# Create a DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(featuresCol='indexedFeatures')\n",
    "dt.setLabelCol('PE') \\\n",
    "  .setPredictionCol('Predicted_PE') \\\n",
    "  .setFeaturesCol('features') \\\n",
    "  .setMaxBins(100)\n",
    "    \n",
    "dtPipeline = Pipeline()\n",
    "dtPipeline.setStages([vectorizer, dt])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead guessing what parameters to use, we will use [Model Selection](https://spark.apache.org/docs/1.6.2/ml-tuning.html#model-selection-aka-hyperparameter-tuning) or [Hyperparameter Tuning](https://spark.apache.org/docs/1.6.2/ml-tuning.html#model-selection-aka-hyperparameter-tuning) to create the best model.\n",
    "\n",
    "We can reuse the exiting [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation) by replacing the Estimator with our new `dtPipeline` (the number of folds remains 3).\n",
    "\n",
    "### Exercise 7(d)\n",
    "* Use [ParamGridBuilder](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder) to build a parameter grid with the parameter dt.maxDepth and a list of the values 2 and 3, and add the grid to the [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation)\n",
    "* Run the [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation) to find the parameters that yield the best model (i.e., lowest RMSE) and return the best model.\n",
    "\n",
    "Note that it will take some time to run the [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation) as it will run almost 50 Spark jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just reuse our CrossValidator with the new dtPipeline,  RegressionEvaluator regEval, and 3 fold cross validation\n",
    "crossval.setEstimator(dtPipeline)\n",
    "\n",
    "# Let's tune over our dt.maxDepth parameter on the values 2 and 3, create a paramter grid using the ParamGridBuilder\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [2, 3])\n",
    "             .build())\n",
    "\n",
    "# Add the grid to the CrossValidator\n",
    "crossval.setEstimatorParamMaps(paramGrid)\n",
    "\n",
    "# Now let's find and return the best model\n",
    "dtModel = crossval.fit(trainingSetDF).bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7(e)\n",
    "Now let's see how our tuned DecisionTreeRegressor model's RMSE and $r^2$ values compare to our tuned LinearRegression model.\n",
    "\n",
    "Complete and run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use dtModel to compute an evaluation metric for our test dataset: testSetDF\n",
    "predictionsAndLabelsDF = dtModel.transform(testSetDF).select('AT', 'V', 'AP', 'RH', 'PE', 'Predicted_PE')\n",
    "\n",
    "# Run the previously created RMSE evaluator, regEval, on the predictionsAndLabelsDF DataFrame\n",
    "rmseDT = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: 'rmse'})\n",
    "\n",
    "# Now let's compute the r2 evaluation metric for our test dataset\n",
    "r2DT = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: 'r2'})\n",
    "\n",
    "print('LR Root Mean Square Error: {0:.2f}'.format(rmseNew))\n",
    "print('DT Root Mean Square Error: {0:.2f}'.format(rmseDT))\n",
    "print('LR r2: {0:.2f}'.format(r2New))\n",
    "print('DT r2: {0:.2f}'.format(r2DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line below will pull the Decision Tree model from the Pipeline as display it as an if-then-else string. Again, we have to \"reach through\" to the JVM API to make this one work.\n",
    "\n",
    "**ToDo**: Run the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtModel.stages[-1]._java_obj.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our DecisionTree has slightly worse RMSE than our LinearRegression model (LR: 4.59 vs DT: 5.19). Maybe we can try an [Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning) method such as [Gradient-Boosted Decision Trees](https://en.wikipedia.org/wiki/Gradient_boosting) to see if we can strengthen our model by using an ensemble of weaker trees with weighting to reduce the error in our model.\n",
    "\n",
    "[Random forests](https://en.wikipedia.org/wiki/Random_forest) or random decision tree forests are an ensemble learning method for regression that operate by constructing a multitude of decision trees at training time and outputting the class that is the mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n",
    "\n",
    "Spark ML Pipeline provides [RandomForestRegressor()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor) as an implementation of [Random forests](https://en.wikipedia.org/wiki/Random_forest).\n",
    "\n",
    "The cell below is based on the [Spark ML Pipeline API for Random Forest Regressor](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7(f)\n",
    "* Read the [Random Forest Regressor](https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor) documentation\n",
    "* In the next cell, create a [RandomForestRegressor()](https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor)\n",
    "* The next step is to set the parameters for the method (we do this for you):\n",
    "    * Set the name of the prediction column to \"Predicted_PE\"\n",
    "    * Set the name of the features column to \"features\"\n",
    "    * Set the random number generator seed to 100088121L\n",
    "    * Set the maximum depth to 8\n",
    "    * Set the number of trees to 30\n",
    "* Create the [ML Pipeline](https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.Pipeline) and set the stages to the Vectorizer we created earlier and [RandomForestRegressor()](https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor) learner we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Create a RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "rf.setLabelCol('PE') \\\n",
    "  .setPredictionCol('Predicted_PE') \\\n",
    "  .setFeaturesCol('features') \\\n",
    "  .setSeed(100088121L) \\\n",
    "  .setMaxDepth(8) \\\n",
    "  .setNumTrees(30)\n",
    "\n",
    "# Create a Pipeline\n",
    "rfPipeline = Pipeline()\n",
    "\n",
    "# Set the stages of the Pipeline\n",
    "rfPipeline.setStages([vectorizer, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Decision Trees, instead guessing what parameters to use, we will use [Model Selection](https://spark.apache.org/docs/2.1.0/ml-tuning.html#model-selection-aka-hyperparameter-tuning) or [Hyperparameter Tuning](https://spark.apache.org/docs/2.1.0/ml-tuning.html#model-selection-aka-hyperparameter-tuning) to create the best model.\n",
    "\n",
    "We can reuse the exiting [CrossValidator](https://spark.apache.org/docs/2.1.0/ml-tuning.html#cross-validation) by replacing the Estimator with our new rfPipeline (the number of folds remains 3).\n",
    "\n",
    "### Exercise 7(g)\n",
    "* Use [ParamGridBuilder](https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder) to build a parameter grid with the parameter rf.maxBins and a list of the values 50 and 100, and add the grid to the CrossValidator\n",
    "* Run the [CrossValidator](https://spark.apache.org/docs/2.1.0/ml-tuning.html#cross-validation) to find the parameters that yield the best model (i.e., lowest RMSE) and return the best model.\n",
    "\n",
    "_Note that it will take some time to run the [CrossValidator](https://spark.apache.org/docs/2.1.0/ml-tuning.html#cross-validation) as it will run almost 100 Spark jobs, and each job takes longer to run than the prior CrossValidator runs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just reuse our CrossValidator with the new rfPipeline,  RegressionEvaluator regEval, and 3 fold cross validation\n",
    "crossval.setEstimator(rfPipeline)\n",
    "\n",
    "# Let's tune over our rf.maxBins parameter on the values 50 and 100, create a paramter grid using the ParamGridBuilder\n",
    "paramGrid = paramGrid = (ParamGridBuilder()\n",
    "                         .addGrid(rf.maxBins, [50,100])\n",
    "                         .build())\n",
    "\n",
    "# Add the grid to the CrossValidator\n",
    "crossval.setEstimatorParamMaps(paramGrid)\n",
    "\n",
    "# Now let's find and return the best model\n",
    "rfModel = crossval.fit(trainingSetDF).bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7(h)\n",
    "Now let's see how our tuned RandomForestRegressor model's RMSE and $r^2$ values compare to our tuned LinearRegression and tuned DecisionTreeRegressor models.\n",
    "\n",
    "Complete and run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use rfModel to compute an evaluation metric for our test dataset: testSetDF\n",
    "predictionsAndLabelsDF = rfModel.transform(testSetDF)\n",
    "\n",
    "# Run the previously created RMSE evaluator, regEval, on the predictionsAndLabelsDF DataFrame\n",
    "rmseRF = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: 'rmse'})\n",
    "\n",
    "# Now let's compute the r2 evaluation metric for our test dataset\n",
    "r2RF = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: 'r2'})\n",
    "\n",
    "print('LR Root Mean Square Error: {0:.2f}'.format(rmseNew))\n",
    "print('DT Root Mean Square Error: {0:.2f}'.format(rmseDT))\n",
    "print('RF Root Mean Square Error: {0:.2f}'.format(rmseRF))\n",
    "print('LR r2: {0:.2f}'.format(r2New))\n",
    "print('DT r2: {0:.2f}'.format(r2DT))\n",
    "print('RF r2: {0:.2f}'.format(r2RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line below will pull the Random Forest model from the Pipeline as display it as an if-then-else string.\n",
    "\n",
    "**ToDo**: Run the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfModel.stages[-1]._java_obj.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
